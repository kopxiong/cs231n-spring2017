1. Pros and Cons of Nearest Neighbor classifier.
Cons:
a. The classifier must remember all of the training data and store it for future comparisons with the test data.
   This is space inefficient because datasets may easily be gigabytes in size.
b. Classifying a test image is expensive since it requires a comparison to all training images.

2. Approximate Nearest Neighbors (ANN)

3. As alluded to in the previous section

4. Activation function:
a. Sigmoid
(+) Nice interpretation as the firing rate of a neuron:
from not firing at all (0) to fully-saturated firing at an assumed maximum frequency (1)
(-) Sigmoids saturate and kill gradients
(-) Sigmoid outputs are not zero-centered
b. Tanh
A scaled sigmoid function, but zero-centered.
c. ReLU
(+) Greatly accelerates the convergence of SGD
(+) Less expensive operations
(-) ReLU units can be fragile during training and can “die”
d. Leaky ReLU ------>  PReLU (Parametric Rectified Linear Unit)
e. Maxout (Maxout Networks https://arxiv.org/pdf/1302.4389.pdf)
(+) Generalizes the ReLU and its leaky version
(-) Doubled number of parameters

5. Larger Neural Networks can represent more complicated functions.
Neural Networks (usually with non-linear activation functions) are non-convex.

6. Difference between *args and **kwargs, see
https://stackoverflow.com/questions/3394835/args-and-kwargs,
*args:    positional arguments
**kwargs: keyword arguments,      example: captioning_solver.py

def func(required_arg, *args, **kwargs):
    # required_arg is a positional-only parameter.
    print required_arg

    # args is a tuple of positional arguments,
    # because the parameter name has * prepended.
    if args: # If args is not empty.
        print args

    # kwargs is a dictionary of keyword arguments,
    # because the parameter name has ** prepended.
    if kwargs: # If kwargs is not empty.
        print kwargs

>>> func()
Traceback (most recent call last):
  File "<stdin>", line 1, in <module>
TypeError: func() takes at least 1 argument (0 given)

>>> func("required argument")
required argument

>>> func("required argument", 1, 2, '3')
required argument
(1, 2, '3')

>>> func("required argument", 1, 2, '3', keyword1=4, keyword2="foo")
required argument
(1, 2, '3')
{'keyword2': 'foo', 'keyword1': 4}

7. A learning algorithm that can reduce the chance of fitting noise is called robust.
L1 loss function is robust, unstable solution, possibly multiple solutions.
L2 loss function is not very robust, stable solution, always one solution.

8. Regularization is a very important technique in machine learning to prevent overfitting. 
L1 regularization: Computational inefficient in non-sparse cases, sparse outputs, built-in feature selection.
L2 regularization: Computational efficient due to have analytic solutions, non-sparse outputs, no feature selection.
Max norm constraints
Dropout

9. Linear regression vs Logistic regression
